import{a as R,t as j}from"./disclose-version.Cuai9WB4.js";import"./legacy.CuMp8f7V.js";import{p as F,s as e,T as K,t as i,a as H,c as t,r as a,S as P}from"./runtime.BjFdoZql.js";import{s as o}from"./attributes.ClXRwxjr.js";import{i as W}from"./lifecycle.D83fNPOv.js";import{g as z}from"./index.ogBiGyk8.js";const B=""+new URL("../assets/image.UPuWNdFG.png",import.meta.url).href,E=""+new URL("../assets/image-1.BaZAEMX-.png",import.meta.url).href,C=""+new URL("../assets/image-2.D06sQsGN.png",import.meta.url).href,G=""+new URL("../assets/image-3.C5SN--D8.png",import.meta.url).href,Z={title:"Deep dive into LLMs with Andrej Karphaty",description:"Notes by video from A. Karphaty",date:"2025-02-13",categories:["programming","ai"],published:!0,language:"en"};var U=j(`<p>Here are some notes taken while wotching the video on LLMs created by Andrej Karphathy. Thank you, Andrej!</p> <ul><li><p>The video itself: <a rel="nofollow">https://www.youtube.com/watch?v=7xTGNNLPyMI&ab_channel=AndrejKarpathy</a></p></li> <li><p>How data is being tokenized: <a rel="nofollow">https://tiktokenizer.vercel.app/</a></p></li> <li><p>Visualisation of CNNs: <a rel="nofollow">https://bbycroft.net/llm</a></p></li></ul> <h2>Initial training</h2> <p>It involves data crawling, parsing, clearing out. Then tokenizing and feeding into CNN. The weights are adjusted and as a result we have BASE or completion models. This process is extremely computational intensive.</p> <h2>*-instruct models</h2> <p>Means that they are not just BASE models but also were taught by huge set of “few-shot” conversations created by human experts and labellers.</p> <h2>Few-shot learning</h2> <p>Ball: Ball, Dog: Hund, Cat: …</p> <p>Here LLM adds “Katze” based on the few shots.</p> <h2>“Vague recollection” vs. “Working memory”</h2> <p>Knowledge in the parameters == Vague recollection (e.g. of something you read 1 month ago)
Knowledge in the tokens of the context window == Working memory.</p> <p>Working memory is usually done by pre-feeding (copy&paste) data in the general prompt or by RAG solutions.</p> <h2>LLM’s Self</h2> <p>It is input-output pure function, no self! Each times it starts, runs its statistic/stochastic stuff and then shuts off.</p> <p>During the fine-tuning it gets this “self” during fine-tuning or based on the similar data in the internet. So it can hallucinate easily.</p> <p>Still, you can program this out.</p> <h2>It needs tokens to “think”</h2> <p><img alt="image.png"></p> <p>Left is significantly worse! It gives immediately the answer. That is totally bad for training. It reads and generates tokens from left to right.</p> <p>On the right, we show ongoing process, we create step-by-step calculations.</p> <p>So it builds its own context window, building tokens one by one, and helping itself to come to the answer.</p> <h3>Hint by Andrej — ask to lean on tools</h3> <p>For such kind of questions it is better to ask “use code”, because it would do a solution more reliable. LLM uses a tool, it is not relying on its mental arithmetic.</p> <p>You can also ask “use code interpreter” or “use web search”.</p> <p><strong>Lean on tools whenever it is possible.</strong></p> <p>Model is not good at thinking. Model is good at copy&pasting.</p> <h3>Models do not think</h3> <p>Models can’t count.</p> <p>Models do not see letters, characters, they are not good at spelling now. They usually have tokens that may include many char-s within single token.</p> <h3>SFT model</h3> <p>Supervised fine-tuned model. It is the result of this post-training step. It kinda mimics the expert’s solutions, statistically.</p> <h2>Reinforcement learning</h2> <p>Next step in post-learning process, based on SFT model. It goes beyond mimics and goes to reasoning.</p> <p><img alt="image.png"></p> <p>So it is kinda we get “pre-training” by reading the theory, then we have worked problems resolved by a professional (author of the book) to learn how to solve the problems.</p> <p>Then, we can have a task to resolve and the final answer to compare with ours result, but solution we build on our own. And here we go with reinforcement.</p> <p>Our knowledge is not LLMs knowledge. But we (human experts / labellers) give a hint of our cognition by providing samples.</p> <p>We need to encourage solutions that lead to correct answers.</p> <p>We decide which is the best and then feed it to the model.</p> <p><img alt="image.png"></p> <h3>RL (”thinking”) Model</h3> <p>It is reinforcement learning model.</p> <p>It is about refrain, backtrack, reevaluate — it is so-called chain-of-thought. The model discovers the way to think.</p> <p>Check the result of different perspectives. It is not hardcoded or shown by example. We just give the answer and it tries to find the most effective solution.</p> <p>Another name is “reasoning” model. DeepSeek was first who has represented this into a wide public.</p> <h2><a rel="nofollow">together.ai</a></h2> <p>The resource you can use to use deployed models not by main players. E.g. you can talk to DeepSeek R1 without interacting with their actual hosts (as a company). You can also do it in Azure, still it is not as user-friendly as <a rel="nofollow">https://together.ai</a></p> <h3>Reinforcement learning in un-verifiable domains ⇒ RLHF (Reinforcement learning with Human Feedback)</h3> <p>E.g. jokes, humour, poems. There is a problem to score well so “LLM Judge” is not so helpful, so we use human feedback.</p> <p>Neural Network (reward model, complete separate neural network) can build based on that simulation of human preferences.</p> <p>So, we’re slightly nudging weights iteratively until it aligns well with human scores.</p> <h3>Discriminator — generator gap</h3> <p>It is easier for human to discriminate than to generate. So, human labeller would rather pick one of the best responses (poem .e.g.) instead of writing his own one.</p> <h3>Downside</h3> <p>It can still go wrong as RN can find a way to “outsmart” the scoring. So it can find a way to trick the scoring model and bring shitty results.</p> <p><img alt="image.png"></p> <h2>Something is good, something is not</h2> <p>At some tasks LLMs are great, at some tasks they are not so good. Keep in mind, they are tools.</p>`,1);function $(y,k){F(k,!1);const _=z(),[n,V]=_;W();var u=U(),s=e(K(u),2),r=t(s),c=t(r),L=e(t(c));i(()=>o(L,"href",n("https://www.youtube.com/watch?v=7xTGNNLPyMI&ab_channel=AndrejKarpathy",void 0))),a(c),a(r);var l=e(r,2),f=t(l),I=e(t(f));i(()=>o(I,"href",n("https://tiktokenizer.vercel.app/",void 0))),a(f),a(l);var w=e(l,2),b=t(w),N=e(t(b));i(()=>o(N,"href",n("https://bbycroft.net/llm",void 0))),a(b),a(w),a(s);var p=e(s,32),D=t(p);o(D,"src",B),a(p);var h=e(p,32),M=t(h);o(M,"src",E),a(h);var d=e(h,12),S=t(d);o(S,"src",C),a(d);var g=e(d,12),T=t(g);i(()=>o(T,"href",n("http://together.ai",void 0))),a(g);var m=e(g,2),A=e(t(m));i(()=>o(A,"href",n("https://together.ai",void 0))),a(m);var v=e(m,18),x=t(v);o(x,"src",G),a(v),P(4),R(y,u),H()}export{$ as default,Z as metadata};
