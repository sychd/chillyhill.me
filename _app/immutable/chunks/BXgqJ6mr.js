import"./Bzak7iHL.js";import"./CiPh4DN7.js";import{p as H,f as P,s as e,_ as W,t as z,a as B,b as E,c as t,r as o,$ as C}from"./CHd-ugWw.js";import{s as a}from"./Cf8sFkov.js";import{i as G}from"./B4ZchNSr.js";import{g as U}from"./vA9fPZZb.js";const V=""+new URL("../assets/image.UPuWNdFG.png",import.meta.url).href,O=""+new URL("../assets/image-1.BaZAEMX-.png",import.meta.url).href,Y=""+new URL("../assets/image-2.D06sQsGN.png",import.meta.url).href,q=""+new URL("../assets/image-3.C5SN--D8.png",import.meta.url).href,J={title:"Deep dive into LLMs with Andrej Karphaty",description:"Notes by video from A. Karphaty",date:"2025-02-13",categories:["programming","ai"],published:!0,language:"en"},{title:ae,description:ne,date:ie,categories:se,published:re,language:le}=J;var Q=P(`<p>Here are some notes taken while wotching the video on LLMs created by Andrej Karphathy. Thank you, Andrej!</p> <ul><li><p>The video itself: <a rel="nofollow">https://www.youtube.com/watch?v=7xTGNNLPyMI&ab_channel=AndrejKarpathy</a></p></li> <li><p>How data is being tokenized: <a rel="nofollow">https://tiktokenizer.vercel.app/</a></p></li> <li><p>Visualisation of CNNs: <a rel="nofollow">https://bbycroft.net/llm</a></p></li></ul> <h2>Initial training</h2> <p>It involves data crawling, parsing, clearing out. Then tokenizing and feeding into CNN. The weights are adjusted and as a result we have BASE or completion models. This process is extremely computational intensive.</p> <h2>*-instruct models</h2> <p>Means that they are not just BASE models but also were taught by huge set of “few-shot” conversations created by human experts and labellers.</p> <h2>Few-shot learning</h2> <p>Ball: Ball, Dog: Hund, Cat: …</p> <p>Here LLM adds “Katze” based on the few shots.</p> <h2>“Vague recollection” vs. “Working memory”</h2> <p>Knowledge in the parameters == Vague recollection (e.g. of something you read 1 month ago)
Knowledge in the tokens of the context window == Working memory.</p> <p>Working memory is usually done by pre-feeding (copy&paste) data in the general prompt or by RAG solutions.</p> <h2>LLM’s Self</h2> <p>It is input-output pure function, no self! Each times it starts, runs its statistic/stochastic stuff and then shuts off.</p> <p>During the fine-tuning it gets this “self” during fine-tuning or based on the similar data in the internet. So it can hallucinate easily.</p> <p>Still, you can program this out.</p> <h2>It needs tokens to “think”</h2> <p><img alt="image.png"/></p> <p>Left is significantly worse! It gives immediately the answer. That is totally bad for training. It reads and generates tokens from left to right.</p> <p>On the right, we show ongoing process, we create step-by-step calculations.</p> <p>So it builds its own context window, building tokens one by one, and helping itself to come to the answer.</p> <h3>Hint by Andrej — ask to lean on tools</h3> <p>For such kind of questions it is better to ask “use code”, because it would do a solution more reliable. LLM uses a tool, it is not relying on its mental arithmetic.</p> <p>You can also ask “use code interpreter” or “use web search”.</p> <p><strong>Lean on tools whenever it is possible.</strong></p> <p>Model is not good at thinking. Model is good at copy&pasting.</p> <h3>Models do not think</h3> <p>Models can’t count.</p> <p>Models do not see letters, characters, they are not good at spelling now. They usually have tokens that may include many char-s within single token.</p> <h3>SFT model</h3> <p>Supervised fine-tuned model. It is the result of this post-training step. It kinda mimics the expert’s solutions, statistically.</p> <h2>Reinforcement learning</h2> <p>Next step in post-learning process, based on SFT model. It goes beyond mimics and goes to reasoning.</p> <p><img alt="image.png"/></p> <p>So it is kinda we get “pre-training” by reading the theory, then we have worked problems resolved by a professional (author of the book) to learn how to solve the problems.</p> <p>Then, we can have a task to resolve and the final answer to compare with ours result, but solution we build on our own. And here we go with reinforcement.</p> <p>Our knowledge is not LLMs knowledge. But we (human experts / labellers) give a hint of our cognition by providing samples.</p> <p>We need to encourage solutions that lead to correct answers.</p> <p>We decide which is the best and then feed it to the model.</p> <p><img alt="image.png"/></p> <h3>RL (”thinking”) Model</h3> <p>It is reinforcement learning model.</p> <p>It is about refrain, backtrack, reevaluate — it is so-called chain-of-thought. The model discovers the way to think.</p> <p>Check the result of different perspectives. It is not hardcoded or shown by example. We just give the answer and it tries to find the most effective solution.</p> <p>Another name is “reasoning” model. DeepSeek was first who has represented this into a wide public.</p> <h2><a rel="nofollow">together.ai</a></h2> <p>The resource you can use to use deployed models not by main players. E.g. you can talk to DeepSeek R1 without interacting with their actual hosts (as a company). You can also do it in Azure, still it is not as user-friendly as <a rel="nofollow">https://together.ai</a></p> <h3>Reinforcement learning in un-verifiable domains ⇒ RLHF (Reinforcement learning with Human Feedback)</h3> <p>E.g. jokes, humour, poems. There is a problem to score well so “LLM Judge” is not so helpful, so we use human feedback.</p> <p>Neural Network (reward model, complete separate neural network) can build based on that simulation of human preferences.</p> <p>So, we’re slightly nudging weights iteratively until it aligns well with human scores.</p> <h3>Discriminator — generator gap</h3> <p>It is easier for human to discriminate than to generate. So, human labeller would rather pick one of the best responses (poem .e.g.) instead of writing his own one.</p> <h3>Downside</h3> <p>It can still go wrong as RN can find a way to “outsmart” the scoring. So it can find a way to trick the scoring model and bring shitty results.</p> <p><img alt="image.png"/></p> <h2>Something is good, something is not</h2> <p>At some tasks LLMs are great, at some tasks they are not so good. Keep in mind, they are tools.</p>`,1);function pe(v,y){H(y,!1);const k=U(),[n]=k;G();var m=Q(),i=e(W(m),2),s=t(i),u=t(s),_=e(t(u));o(u),o(s);var r=e(s,2),c=t(r),L=e(t(c));o(c),o(r);var f=e(r,2),w=t(f),I=e(t(w));o(w),o(f),o(i);var l=e(i,32),N=t(l);o(l);var p=e(l,32),D=t(p);o(p);var h=e(p,12),M=t(h);o(h);var d=e(h,12),S=t(d);o(d);var g=e(d,2),A=e(t(g));o(g);var b=e(g,18),T=t(b);o(b),C(4),z((x,R,j,F,K)=>{a(_,"href",x),a(L,"href",R),a(I,"href",j),a(N,"src",V),a(D,"src",O),a(M,"src",Y),a(S,"href",F),a(A,"href",K),a(T,"src",q)},[()=>n("https://www.youtube.com/watch?v=7xTGNNLPyMI&ab_channel=AndrejKarpathy",void 0),()=>n("https://tiktokenizer.vercel.app/",void 0),()=>n("https://bbycroft.net/llm",void 0),()=>n("http://together.ai",void 0),()=>n("https://together.ai",void 0)]),B(v,m),E()}export{pe as default,J as metadata};
